{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28489128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b42414",
   "metadata": {},
   "source": [
    "### Contextual bandit problem - reward dynamics are state dependent\n",
    "\n",
    "In this case, there is no universally \"best\" option - instead we have user preferences, which are as follows:\n",
    "1. Mobile users prefer `a0`.\n",
    "2. Desktop weekend traffic prefers `a1`.\n",
    "3. Desktop weekday morning traffic prefers `a2`.\n",
    "4. Desktop weekday non-morning traffic prefers `a3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0c5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_mobile_df = pd.DataFrame({'is_mobile': [0, 1], 'key': [0, 0]})\n",
    "is_weekend_df = pd.DataFrame({'is_weekend': [0, 1], 'key': [0, 0]})\n",
    "is_morning_df = pd.DataFrame({'is_morning': [0, 1], 'key': [0, 0]})\n",
    "action_df = pd.DataFrame({'action': ['a0', 'a1', 'a2', 'a3'], 'key': [0, 0, 0, 0]})\n",
    "dynamics_df = is_mobile_df.merge(is_weekend_df).merge(is_morning_df).merge(action_df)\n",
    "dynamics_df = dynamics_df.drop(columns='key')\n",
    "dynamics_df['reward_rate'] = pd.Series([0.02, 0.02, 0.02, 0.04,\n",
    "                                        0.02, 0.02, 0.04, 0.02,\n",
    "                                        0.02, 0.04, 0.02, 0.02,\n",
    "                                        0.02, 0.04, 0.02, 0.02,\n",
    "                                        0.04, 0.02, 0.02, 0.02,\n",
    "                                        0.04, 0.02, 0.02, 0.02,\n",
    "                                        0.04, 0.02, 0.02, 0.02,\n",
    "                                        0.04, 0.02, 0.02, 0.02,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ebb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(batch_size):\n",
    "    random_row = dynamics_df.sample(n=batch_size, replace=True, axis=0)\n",
    "    random_row = random_row.drop(columns=['action', 'reward_rate'])\n",
    "    return random_row.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39581743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards(state_df, action_df, dynamics_df):\n",
    "    sa_df = state_df.merge(action_df, left_index=True, right_index=True)\n",
    "    sarr_df = sa_df.merge(dynamics_df)\n",
    "    sarr_df['reward'] = np.random.binomial(n=1, p=sarr_df['reward_rate'])\n",
    "    sar_df = sarr_df.drop(columns='reward_rate')\n",
    "    return sar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc524dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c632b679",
   "metadata": {},
   "source": [
    "#### Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f08c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "cumulative_reward = 0\n",
    "for _ in range(num_batches):\n",
    "    state = get_state(batch_size)\n",
    "    action = pd.DataFrame({'action': np.random.choice(['a0', 'a1', 'a2', 'a3'], size=batch_size)})\n",
    "    sar_df = get_rewards(state, action, dynamics_df)\n",
    "    cumulative_reward += sar_df['reward'].sum()\n",
    "print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb5309",
   "metadata": {},
   "source": [
    "#### Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82905349",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tx = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('categorical', cat_tx, ['action'])\n",
    "])\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=1000)\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517751fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['is_mobile', 'is_weekend', 'is_morning', 'action']\n",
    "cumulative_reward = 0\n",
    "first_batch = True\n",
    "history = pd.DataFrame({'is_mobile': [], 'is_weekend': [], 'is_morning': [], 'action': [], 'reward': []})\n",
    "for _ in range(num_batches):\n",
    "    if first_batch:\n",
    "        state = get_state(batch_size)\n",
    "        action = pd.DataFrame({'action': np.random.choice(['a0', 'a1', 'a2', 'a3'], size=batch_size)})\n",
    "        sar_df = get_rewards(state, action, dynamics_df)\n",
    "        history = pd.concat([history, sar_df]).reset_index(drop=True)\n",
    "        cumulative_reward += sar_df['reward'].sum()\n",
    "        first_batch = False\n",
    "    else:\n",
    "        #Learn on the history\n",
    "        model_pipeline = pipeline.fit(history[col_names], history['reward'])\n",
    "        #Get new state\n",
    "        state = get_state(batch_size)\n",
    "        #Get greedy actions\n",
    "        state['key'] = 0\n",
    "        state['row_num'] = range(batch_size)\n",
    "        action_choice_df = pd.DataFrame({'action': ['a0', 'a1', 'a2', 'a3'], 'key': 0})\n",
    "        input_df = state.merge(action_choice_df)\n",
    "        input_df['q_value'] = model_pipeline.predict_proba(input_df[col_names])[:,1]\n",
    "        ba_df = input_df.loc[input_df.groupby(['row_num'])['q_value'].idxmax()].set_index('row_num')\n",
    "        ba_df = ba_df.drop(columns=['key', 'q_value'])\n",
    "        actions = ba_df[['action']].copy()\n",
    "        #Add in exploration\n",
    "        random_indices = list(np.random.choice(range(batch_size), size=int(batch_size/10)))\n",
    "        for i in random_indices:\n",
    "            actions.loc[i, 'action'] = np.random.choice(['a0', 'a1', 'a2', 'a3'])\n",
    "        #Interact with the environment\n",
    "        sar_df = get_rewards(state, actions, dynamics_df)\n",
    "        history = pd.concat([history, sar_df]).reset_index(drop=True)\n",
    "        cumulative_reward += sar_df['reward'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d251125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
